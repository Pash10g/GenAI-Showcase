{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 🎬 AI-Powered Real-time Video Stream Intelligence & Incident Detection System\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mongodb-developer/GenAI-Showcase/blob/main/notebooks/agents/video_intelligence_agent.ipynb)\n",
        "\n",
        "[![AI Learning Hub](https://img.shields.io/badge/AI%20Learning%20Hub-Click%20Here-blue)](https://www.mongodb.com/events/webinar/building-advanced-gen-ai-apps-with-gravity9-and-mongodb)\n",
        "\n",
        "An intelligent video monitoring system that provides real-time analysis and incident detection for live video streams, broadcasts, and recordings. The system combines  AI embeddings, and languge models to automatically detect, analyze, and resolve video quality issues, network problems, and streaming incidents as they occur.\n",
        "\n",
        "This notebook implements an **AI-Powered Real-time Video Stream Intelligence & Incident Detection System** that automatically monitors and analyzes video content to detect incidents, quality issues, and network problems. The system extracts frames from videos at regular intervals, generates semantic embeddings using Voyage AI's multimodal models, and creates detailed scene descriptions using OpenAI's GPT-4 Vision. \n",
        "\n",
        "- **Users can search through video content using natural language queries like \"find frames with a referee\" and instantly jump to relevant timestamps through an interactive HTML5 video player that displays similarity scores and scene descriptions.**\n",
        "\n",
        "The system supports multiple video sources including local files, webcams, and YouTube livestreams, storing all analysis data in MongoDB with vector search capabilities for fast retrieval. Built using an agent-based architecture, specialized AI agents handle different aspects like frame retrieval, video display, incident analysis, and stream monitoring. \n",
        "- **Real-time monitoring continuously processes live video feeds, compares frames against a database of known incidents, and provides immediate alerts with visual overlays.**\n",
        "\n",
        "This makes the system valuable for broadcast monitoring, security surveillance, quality assurance, and content discovery applications where organizations need to automatically detect issues, search large video archives, or monitor live streams for technical problems or security incidents.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "! pip install -Uq pymongo voyageai pandas datasets opencv-python pillow openai openai-agents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "\n",
        "# Function to securely get and set environment variables\n",
        "def set_env_securely(var_name, prompt):\n",
        "    value = getpass.getpass(prompt)\n",
        "    os.environ[var_name] = value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Extracting Embeddings and Metadata from Video Data\n",
        "\n",
        "This step involves extracting video frames and generating corresponding embeddings and metadata descriptions to facilitate intelligent search functionality.\n",
        "\n",
        "This step covers three key techniques:\n",
        "\n",
        "- Video-to-frame conversion for image extraction\n",
        "- Multimodal embedding generation with Voyage AI to encode semantic relationships between text and images\n",
        "- Automated metadata generation using GPT-4o Vision Pro\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.1 Video to Images Function Explanation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "This function: `video_to_images` extracts still images from a video at regular time intervals (default: every 2 seconds).\n",
        "\n",
        "What it does:\n",
        "\n",
        "1. Opens a video file using OpenCV\n",
        "2. Calculates which frames to extract based on the video's frame rate and desired time interval\n",
        "3. Loops through the video, saving only the frames that match the timing interval\n",
        "4. Saves each extracted frame as a JPEG with a timestamp filename (e.g., \"frame_0001_t2.0s.jpg\")\n",
        "5. Returns the total number of frames extracted\n",
        "\n",
        "Key parameters:\n",
        "\n",
        "- `video_path`: Input video file\n",
        "- `output_dir`: Where to save the images (default: \"frames\")  \n",
        "- `interval_seconds`: Time between extractions (default: 2 seconds)\n",
        "\n",
        "Usage:\n",
        "\n",
        "The example usage extracts frames every 2 seconds from \"videos/video.mp4\" and saves them to a \"frames\" folder. This is useful for creating video thumbnails or analyzing video content frame by frame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "import cv2\n",
        "\n",
        "\n",
        "def video_to_images(video_path, output_dir=\"frames\", interval_seconds=2):\n",
        "    \"\"\"\n",
        "    Convert a video to images by extracting frames every specified interval.\n",
        "\n",
        "    Args:\n",
        "        video_path (str): Path to the input video file\n",
        "        output_dir (str): Directory to save extracted frames (default: \"frames\")\n",
        "        interval_seconds (float): Time interval between frame extractions (default: 2 seconds)\n",
        "\n",
        "    Returns:\n",
        "        int: Number of frames extracted\n",
        "    \"\"\"\n",
        "\n",
        "    # Create output directory if it doesn't exist\n",
        "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Open the video file\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    if not cap.isOpened():\n",
        "        raise ValueError(f\"Error: Could not open video file {video_path}\")\n",
        "\n",
        "    # Get video properties\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    duration = total_frames / fps\n",
        "\n",
        "    print(f\"Video info: {fps:.2f} FPS, {total_frames} frames, {duration:.2f} seconds\")\n",
        "\n",
        "    # Calculate frame interval\n",
        "    frame_interval = int(fps * interval_seconds)\n",
        "\n",
        "    frame_count = 0\n",
        "    extracted_count = 0\n",
        "\n",
        "    try:\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            # Extract frame every interval_seconds\n",
        "            if frame_count % frame_interval == 0:\n",
        "                # Create filename with timestamp\n",
        "                timestamp = frame_count / fps\n",
        "                filename = f\"frame_{extracted_count:04d}_t{timestamp:.1f}s.jpg\"\n",
        "                filepath = os.path.join(output_dir, filename)\n",
        "\n",
        "                # Save frame as image\n",
        "                cv2.imwrite(filepath, frame)\n",
        "                extracted_count += 1\n",
        "                print(f\"Extracted frame {extracted_count}: {filename}\")\n",
        "\n",
        "            frame_count += 1\n",
        "\n",
        "    finally:\n",
        "        cap.release()\n",
        "\n",
        "    print(f\"Extraction complete! {extracted_count} frames saved to '{output_dir}'\")\n",
        "    return extracted_count"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before running the cell below, be sure to create a folder in the same location as this notebook called \"videos\" and place your video in that folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create directory videos if it doesn't exist\n",
        "if not os.path.exists(\"videos\"):\n",
        "    os.makedirs(\"videos\")\n",
        "\n",
        "video_to_images(video_path=\"videos/video.mp4\", output_dir=\"frames\", interval_seconds=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2 Setting environment variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "set_env_securely(\"VOYAGE_API_KEY\", \"Enter your Voyage API Key: \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "set_env_securely(\"OPENAI_API_KEY\", \"Enter your OpenAI API Key: \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import voyageai\n",
        "from openai import OpenAI\n",
        "\n",
        "# Initialize Voyage AI client\n",
        "# Ensure Voyage API key is set in the environment variables\n",
        "voyageai_client = voyageai.Client()\n",
        "\n",
        "# Initialize OpenAI client\n",
        "# Ensure OpenAI API key is set in the environment variables\n",
        "openai_client = OpenAI()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.3 Generating Embeddings with Voyage AI\n",
        "\n",
        "Voyage AI multimodal-3 is a state-of-the-art embedding model that revolutionizes how we process documents containing interleaved text and images by vectorizing both modalities through a unified transformer backbone, eliminating the need for complex document parsing while improving retrieval accuracy by an average of 19.63% over competing models. \n",
        "\n",
        "**Unlike traditional CLIP-based models that process text and images separately, voyage-multimodal-3 captures the contextual relationships between visual and textual elements in screenshots, PDFs, slides, tables, and figures, making it ideal for RAG applications and semantic search across content-rich documents where visual layout and textual content are equally important.**\n",
        "\n",
        "Learn more on Voyage AI Multimodal embeddings here: https://docs.voyageai.com/docs/multimodal-embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import List\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "def get_voyage_embedding(data: Image.Image | str, input_type: str = \"document\") -> List:\n",
        "    \"\"\"\n",
        "    Get Voyage AI embeddings for images and text.\n",
        "\n",
        "    Args:\n",
        "        data (Image.Image | str): An image or text to embed.\n",
        "        input_type (str): Input type, either \"document\" or \"query\".\n",
        "\n",
        "    Returns:\n",
        "        List: Embeddings as a list.\n",
        "    \"\"\"\n",
        "    embedding = voyageai_client.multimodal_embed(\n",
        "        inputs=[[data]], model=\"voyage-multimodal-3\", input_type=input_type\n",
        "    ).embeddings[0]\n",
        "    return embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "import base64\n",
        "\n",
        "\n",
        "def encode_image_to_base64(image_path: str) -> str:\n",
        "    \"\"\"\n",
        "    Encode an image file to base64 string for OpenAI.\n",
        "\n",
        "    Args:\n",
        "        image_path (str): Path to the image file\n",
        "\n",
        "    Returns:\n",
        "        str: Base64 encoded image string\n",
        "    \"\"\"\n",
        "    with open(image_path, \"rb\") as image_file:\n",
        "        return base64.b64encode(image_file.read()).decode(\"utf-8\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import List, Optional\n",
        "\n",
        "\n",
        "def get_image_embedding(\n",
        "    image_path: str, input_type: str = \"document\"\n",
        ") -> Optional[List[float]]:\n",
        "    \"\"\"\n",
        "    Get embedding for a single image file using Voyage AI.\n",
        "\n",
        "    Args:\n",
        "        image_path (str): Path to the image file\n",
        "        input_type (str): Input type for embedding (\"document\" or \"query\")\n",
        "\n",
        "    Returns:\n",
        "        Optional[List[float]]: Image embedding vector or None if failed\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load image using PIL\n",
        "        image = Image.open(image_path)\n",
        "\n",
        "        # Get embedding using the Voyage AI client\n",
        "        embedding = get_voyage_embedding(image, input_type)\n",
        "\n",
        "        print(\n",
        "            f\"✓ Got embedding for {os.path.basename(image_path)} (dimension: {len(embedding)})\"\n",
        "        )\n",
        "        return embedding\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error getting embedding for {image_path}: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 1.3.1 Generating metadata with each from OpenAI Vision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_frame_description(image_path: str) -> Optional[str]:\n",
        "    \"\"\"\n",
        "    Generate a description of the frame content using OpenAI vision model.\n",
        "\n",
        "    Args:\n",
        "        image_path (str): Path to the image file\n",
        "\n",
        "    Returns:\n",
        "        Optional[str]: Description of the frame content or None if failed\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Encode image to base64\n",
        "        base64_image = encode_image_to_base64(image_path)\n",
        "\n",
        "        # Create the prompt for frame description\n",
        "        response = openai_client.chat.completions.create(\n",
        "            model=\"gpt-4o\",  # Use GPT-4 vision model\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": [\n",
        "                        {\n",
        "                            \"type\": \"text\",\n",
        "                            \"text\": \"Describe what you see in this video frame. Include details about objects, people, actions, setting, and any text visible. Be specific and descriptive but concise.\",\n",
        "                        },\n",
        "                        {\n",
        "                            \"type\": \"image_url\",\n",
        "                            \"image_url\": {\n",
        "                                \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
        "                            },\n",
        "                        },\n",
        "                    ],\n",
        "                }\n",
        "            ],\n",
        "            max_tokens=300,\n",
        "        )\n",
        "\n",
        "        description = response.choices[0].message.content\n",
        "        print(f\"✓ Generated description for {os.path.basename(image_path)}\")\n",
        "        return description\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating description for {image_path}: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.4 Generating frame metadata\n",
        "\n",
        "This code processes video frames to extract both AI embeddings and text descriptions for intelligent search capabilities.\n",
        "\n",
        "**`process_single_frame()`** - Handles individual frame processing:\n",
        "1. Loads an image using PIL\n",
        "2. Generates a vector embedding using Voyage AI (captures visual semantic meaning)\n",
        "3. Creates a text description using OpenAI's vision model\n",
        "4. Returns both as a dictionary or None if processing fails\n",
        "\n",
        "**`process_frames_to_embeddings_with_descriptions()`** - Batch processes all frames:\n",
        "1. **Discovers frames** - Scans the frames directory for image files (.jpg, .png, etc.)\n",
        "2. **Processes sequentially** - Calls `process_single_frame()` for each image\n",
        "3. **Adds metadata** - Extracts frame number and timestamp from filename \n",
        "4. **Rate limiting** - Includes delays between API calls to avoid hitting service limits\n",
        "5. **Progress tracking** - Shows processing status and handles failures gracefully\n",
        "\n",
        "The result is a dictionary mapping each frame filename to its embedding vector, text description, frame number, and timestamp - enabling both visual and semantic search capabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Dict\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "def process_single_frame(\n",
        "    image_path: str, input_type: str = \"document\"\n",
        ") -> Optional[Dict]:\n",
        "    \"\"\"\n",
        "    Process a single frame to get both embedding and description.\n",
        "\n",
        "    Args:\n",
        "        image_path (str): Path to the image file\n",
        "        input_type (str): Input type for embedding (\"document\" or \"query\")\n",
        "\n",
        "    Returns:\n",
        "        Optional[Dict]: Dictionary with 'embedding' and 'frame_description' or None if failed\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load image using PIL for embedding\n",
        "        image = Image.open(image_path)\n",
        "\n",
        "        # Get embedding using Voyage AI\n",
        "        embedding = get_voyage_embedding(image, input_type)\n",
        "\n",
        "        # Generate description using OpenAI\n",
        "        frame_description = generate_frame_description(image_path)\n",
        "\n",
        "        if embedding is not None and frame_description is not None:\n",
        "            return {\"embedding\": embedding, \"frame_description\": frame_description}\n",
        "        else:\n",
        "            print(f\"Failed to process {image_path} - missing embedding or description\")\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing frame {image_path}: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "\n",
        "def process_frames_to_embeddings_with_descriptions(\n",
        "    frames_dir: str = \"frames\",\n",
        "    input_type: str = \"document\",\n",
        "    delay_seconds: float = 0.5,\n",
        "    cut_off_frame: float = 300,\n",
        ") -> Dict[str, Dict]:\n",
        "    \"\"\"\n",
        "    Process all images in frames folder and get both Voyage AI embeddings and OpenAI descriptions.\n",
        "\n",
        "    Args:\n",
        "        frames_dir (str): Directory containing frame images (default: \"frames\")\n",
        "        input_type (str): Input type for embeddings (\"document\" or \"query\")\n",
        "        delay_seconds (float): Delay between API calls to avoid rate limits\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Dict]: Dictionary mapping image filenames to {'embedding': [...], 'frame_description': '...'}\n",
        "    \"\"\"\n",
        "\n",
        "    # Check if frames directory exists\n",
        "    if not os.path.exists(frames_dir):\n",
        "        raise FileNotFoundError(f\"Frames directory '{frames_dir}' not found\")\n",
        "\n",
        "    # Get all image files from the directory\n",
        "    image_extensions = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tiff\", \".webp\"}\n",
        "    image_files = []\n",
        "\n",
        "    for file in os.listdir(frames_dir):\n",
        "        if Path(file).suffix.lower() in image_extensions:\n",
        "            image_files.append(os.path.join(frames_dir, file))\n",
        "\n",
        "    if not image_files:\n",
        "        print(f\"No image files found in '{frames_dir}'\")\n",
        "        return {}\n",
        "\n",
        "    print(f\"Found {len(image_files)} images in '{frames_dir}'\")\n",
        "    print(\"Processing frames for embeddings and descriptions...\")\n",
        "\n",
        "    # Process each image\n",
        "    frame_data = {}\n",
        "    failed_count = 0\n",
        "\n",
        "    for i, image_path in enumerate(sorted(image_files), 1):\n",
        "        print(f\"\\nProcessing {i}/{len(image_files)}: {os.path.basename(image_path)}\")\n",
        "\n",
        "        result = process_single_frame(image_path, input_type)\n",
        "\n",
        "        if result is not None:\n",
        "            # Add the frame number to the frame data\n",
        "            result[\"frame_number\"] = i\n",
        "            # Extract the frame timestamp from the image path \"frame_3538_t6965.0s.jpg\"\n",
        "            result[\"frame_timestamp\"] = float(\n",
        "                os.path.basename(image_path).split(\"_\")[1].split(\".\")[0]\n",
        "            )\n",
        "            frame_data[os.path.basename(image_path)] = result\n",
        "            print(\n",
        "                f\"✓ Complete - Embedding: {len(result['embedding'])}D, Description: {len(result['frame_description'])} chars\"\n",
        "            )\n",
        "        else:\n",
        "            failed_count += 1\n",
        "\n",
        "        # Add delay to respect rate limits (especially for OpenAI)\n",
        "        if i < len(image_files):  # Don't delay after the last image\n",
        "            time.sleep(delay_seconds)\n",
        "\n",
        "        if cut_off_frame is not None and i == cut_off_frame:\n",
        "            break\n",
        "\n",
        "    print(f\"\\n🎉 Completed! Successfully processed {len(frame_data)} frames\")\n",
        "    if failed_count > 0:\n",
        "        print(f\"⚠️ Failed to process {failed_count} frames\")\n",
        "\n",
        "    return frame_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "frame_data = process_frames_to_embeddings_with_descriptions(\n",
        "    frames_dir=\"frames\", input_type=\"document\", delay_seconds=0.5, cut_off_frame=500\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Convert frame data to a pandas dataframe\n",
        "frame_data_df = pd.DataFrame.from_dict(frame_data, orient=\"index\")\n",
        "\n",
        "frame_data_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Connecting and Saving Data To MongoDB "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "set_env_securely(\"MONGODB_URI\", \"Enter your MongoDB URI: \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pymongo\n",
        "\n",
        "\n",
        "def get_mongo_client(mongo_uri):\n",
        "    \"\"\"Establish and validate connection to the MongoDB.\"\"\"\n",
        "\n",
        "    client = pymongo.MongoClient(\n",
        "        mongo_uri, appname=\"devrel.showcase.agents.video_intelligence.python\"\n",
        "    )\n",
        "\n",
        "    # Validate the connection\n",
        "    ping_result = client.admin.command(\"ping\")\n",
        "    if ping_result.get(\"ok\") == 1.0:\n",
        "        # Connection successful\n",
        "        print(\"Connection to MongoDB successful\")\n",
        "        return client\n",
        "    else:\n",
        "        print(\"Connection to MongoDB failed\")\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "DB_NAME = \"video_intelligence\"\n",
        "db_client = get_mongo_client(os.environ.get(\"MONGODB_URI\"))\n",
        "db = db_client[DB_NAME]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 2.1 Collection creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Collection names\n",
        "FRAME_INTELLIGENCE_METADATA = \"video_intelligence\"\n",
        "VIDEO_LIBRARY = \"video_library\"\n",
        "PREVIOUS_FRAME_INCIDENTS = \"previous_frame_incidentS\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Embedding dimensions\n",
        "EMBEDDING_DIMENSIONS = 1024"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create collections\n",
        "\n",
        "\n",
        "def create_collections():\n",
        "    existing_collections = db.list_collection_names()\n",
        "    print(f\"Existing collections: {existing_collections}\")\n",
        "\n",
        "    # If the collection does not exist, create it\n",
        "    if FRAME_INTELLIGENCE_METADATA not in existing_collections:\n",
        "        db.create_collection(FRAME_INTELLIGENCE_METADATA)\n",
        "        print(f\"Created collection: {FRAME_INTELLIGENCE_METADATA}\")\n",
        "    else:\n",
        "        print(f\"Collection {FRAME_INTELLIGENCE_METADATA} already exists\")\n",
        "\n",
        "    if VIDEO_LIBRARY not in existing_collections:\n",
        "        db.create_collection(VIDEO_LIBRARY)\n",
        "        print(f\"Created collection: {VIDEO_LIBRARY}\")\n",
        "    else:\n",
        "        print(f\"Collection {VIDEO_LIBRARY} already exists\")\n",
        "\n",
        "    if PREVIOUS_FRAME_INCIDENTS not in existing_collections:\n",
        "        db.create_collection(PREVIOUS_FRAME_INCIDENTS)\n",
        "        print(f\"Created collection: {PREVIOUS_FRAME_INCIDENTS}\")\n",
        "    else:\n",
        "        print(f\"Collection {PREVIOUS_FRAME_INCIDENTS} already exists\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "create_collections()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 2.2 Creating Vector and Search Indexes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Indexes\n",
        "from pymongo.operations import SearchIndexModel\n",
        "\n",
        "\n",
        "# Create vector search index if it doesn't exist\n",
        "def create_vector_search_index(\n",
        "    collection,\n",
        "    vector_index_name,\n",
        "    dimensions=1024,\n",
        "    quantization=\"scalar\",\n",
        "    embedding_path=\"embedding\",\n",
        "):\n",
        "    # Check if index already exists\n",
        "    try:\n",
        "        existing_indexes = collection.list_search_indexes()\n",
        "        for index in existing_indexes:\n",
        "            if index[\"name\"] == vector_index_name:\n",
        "                print(f\"Vector search index '{vector_index_name}' already exists.\")\n",
        "                return\n",
        "    except Exception as e:\n",
        "        print(f\"Could not list search indexes: {e}\")\n",
        "        return\n",
        "\n",
        "    index_definition = {\n",
        "        \"fields\": [\n",
        "            {\n",
        "                \"type\": \"vector\",\n",
        "                \"path\": embedding_path,\n",
        "                \"numDimensions\": dimensions,\n",
        "                \"similarity\": \"cosine\",\n",
        "            }\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    if quantization == \"scalar\":\n",
        "        index_definition[\"fields\"][0][\"quantization\"] = quantization\n",
        "\n",
        "    if quantization == \"binary\":\n",
        "        index_definition[\"fields\"][0][\"quantization\"] = quantization\n",
        "\n",
        "    # Create vector search index\n",
        "    search_index_model = SearchIndexModel(\n",
        "        definition=index_definition,\n",
        "        name=vector_index_name,\n",
        "        type=\"vectorSearch\",\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        result = collection.create_search_index(model=search_index_model)\n",
        "        print(f\"New search index named '{result}' is building.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating vector search index: {e}\")\n",
        "        return\n",
        "\n",
        "    # Wait for initial sync to complete\n",
        "    print(\n",
        "        f\"Polling to check if the index '{result}' is ready. This may take up to a minute.\"\n",
        "    )\n",
        "    predicate = lambda index: index.get(\"queryable\") is True\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            indices = list(collection.list_search_indexes(result))\n",
        "            if indices and predicate(indices[0]):\n",
        "                break\n",
        "            time.sleep(5)\n",
        "        except Exception as e:\n",
        "            print(f\"Error checking index readiness: {e}\")\n",
        "            time.sleep(5)\n",
        "\n",
        "    print(f\"{result} is ready for querying.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "create_vector_search_index(\n",
        "    db[FRAME_INTELLIGENCE_METADATA], \"vector_search_index_scalar\", quantization=\"scalar\"\n",
        ")\n",
        "create_vector_search_index(\n",
        "    db[FRAME_INTELLIGENCE_METADATA],\n",
        "    \"vector_search_index_full_fidelity\",\n",
        "    quantization=\"full_fidelity\",\n",
        ")\n",
        "create_vector_search_index(\n",
        "    db[FRAME_INTELLIGENCE_METADATA],\n",
        "    \"vector_search_index_full_binary\",\n",
        "    quantization=\"binary\",\n",
        ")\n",
        "create_vector_search_index(\n",
        "    db[PREVIOUS_FRAME_INCIDENTS], \"incident_vector_index_scalar\", quantization=\"scalar\"\n",
        ")\n",
        "create_vector_search_index(\n",
        "    db[VIDEO_LIBRARY], \"video_vector_index\", quantization=\"full_fidelity\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_text_search_index(collection, index_definition, index_name):\n",
        "    \"\"\"\n",
        "    Create a search index for a MongoDB Atlas collection.\n",
        "\n",
        "    Args:\n",
        "    collection: MongoDB collection object\n",
        "    index_definition: Dictionary defining the index mappings\n",
        "    index_name: String name for the index\n",
        "\n",
        "    Returns:\n",
        "    str: Result of the index creation operation\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        search_index_model = SearchIndexModel(\n",
        "            definition=index_definition, name=index_name\n",
        "        )\n",
        "\n",
        "        result = collection.create_search_index(model=search_index_model)\n",
        "        print(f\"Search index '{index_name}' created successfully\")\n",
        "        return result\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating search index: {e!s}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "frame_intelligence_index_definition = {\n",
        "    \"mappings\": {\n",
        "        \"dynamic\": True,\n",
        "        \"fields\": {\n",
        "            # \"video_id\": {\n",
        "            #     \"type\": \"string\",\n",
        "            # },\n",
        "            # \"video_name\": {\n",
        "            #     \"type\": \"string\",\n",
        "            # },\n",
        "            # \"video_url\": {\n",
        "            #     \"type\": \"string\",\n",
        "            # },\n",
        "            \"frame_description\": {\n",
        "                \"type\": \"string\",\n",
        "            },\n",
        "            \"frame_number\": {\n",
        "                \"type\": \"number\",\n",
        "            },\n",
        "            \"frame_timestamp\": {\n",
        "                \"type\": \"date\",\n",
        "            },\n",
        "        },\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "create_text_search_index(\n",
        "    db[FRAME_INTELLIGENCE_METADATA],\n",
        "    frame_intelligence_index_definition,\n",
        "    \"frame_intelligence_index\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Data Ingestion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ensure the collections are empty\n",
        "db[FRAME_INTELLIGENCE_METADATA].delete_many({})\n",
        "db[PREVIOUS_FRAME_INCIDENTS].delete_many({})\n",
        "db[VIDEO_LIBRARY].delete_many({})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "frame_intelligence_documents = frame_data_df.to_dict(orient=\"records\")\n",
        "\n",
        "# Create a new collection for frame intelligence\n",
        "frame_intelligence_collection = db[FRAME_INTELLIGENCE_METADATA]\n",
        "\n",
        "# Insert the frame intelligence documents into the collection\n",
        "frame_intelligence_collection.insert_many(frame_intelligence_documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Retrieval Methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1 Semantic Search powered by Vector Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Implementing vector search\n",
        "def semantic_search_with_mongodb(\n",
        "    user_query, collection, top_n=5, vector_search_index_name=\"vector_search_index\"\n",
        "):\n",
        "    \"\"\"\n",
        "    Perform a vector search in the MongoDB collection based on the user query.\n",
        "\n",
        "    Args:\n",
        "    user_query (str): The user's query string.\n",
        "    collection (MongoCollection): The MongoDB collection to search.\n",
        "    top_n (int): The number of top results to return.\n",
        "    vector_search_index_name (str): The name of the vector search index.\n",
        "\n",
        "    Returns:\n",
        "    list: A list of matching documents.\n",
        "    \"\"\"\n",
        "\n",
        "    # Retrieve the pre-generated embedding for the query from our dictionary\n",
        "    # This embedding represents the semantic meaning of the query as a vector\n",
        "    query_embedding = get_voyage_embedding(user_query, input_type=\"query\")\n",
        "\n",
        "    # Check if we have a valid embedding for the query\n",
        "    if query_embedding is None:\n",
        "        return \"Invalid query or embedding generation failed.\"\n",
        "\n",
        "    # Define the vector search stage using MongoDB's $vectorSearch operator\n",
        "    # This stage performs the semantic similarity search\n",
        "    vector_search_stage = {\n",
        "        \"$vectorSearch\": {\n",
        "            \"index\": vector_search_index_name,  # The vector index we created earlier\n",
        "            \"queryVector\": query_embedding,  # The numerical vector representing our query\n",
        "            \"path\": \"embedding\",  # The field containing document embeddings\n",
        "            \"numCandidates\": 100,  # Explore this many vectors for potential matches\n",
        "            \"limit\": top_n,  # Return only the top N most similar results\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Define which fields to include in the results and their format\n",
        "    project_stage = {\n",
        "        \"$project\": {\n",
        "            \"_id\": 0,  # Exclude MongoDB's internal ID\n",
        "            \"embedding\": 0,\n",
        "            \"score\": {\n",
        "                \"$meta\": \"vectorSearchScore\"  # Include similarity score from vector search\n",
        "            },\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Combine the search and projection stages into a complete pipeline\n",
        "    pipeline = [vector_search_stage, project_stage]\n",
        "\n",
        "    # Execute the pipeline against our collection and get results\n",
        "    results = collection.aggregate(pipeline)\n",
        "\n",
        "    # Convert cursor to a Python list for easier handling\n",
        "    return list(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "user_query = \"Can you get me the frame with the refree on the screen\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "scalar_results = semantic_search_with_mongodb(\n",
        "    user_query=user_query,\n",
        "    collection=db[FRAME_INTELLIGENCE_METADATA],\n",
        "    top_n=5,\n",
        "    vector_search_index_name=\"vector_search_index_scalar\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "scalar_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "full_fidelity_results = semantic_search_with_mongodb(\n",
        "    user_query=user_query,\n",
        "    collection=db[FRAME_INTELLIGENCE_METADATA],\n",
        "    top_n=5,\n",
        "    vector_search_index_name=\"vector_search_index_full_fidelity\",\n",
        ")\n",
        "\n",
        "full_fidelity_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "binary_results = semantic_search_with_mongodb(\n",
        "    user_query=user_query,\n",
        "    collection=db[FRAME_INTELLIGENCE_METADATA],\n",
        "    top_n=5,\n",
        "    vector_search_index_name=\"vector_search_index_binary\",\n",
        ")\n",
        "\n",
        "binary_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 Hybrid Search (Text + Vector Search)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "def hybrid_search(\n",
        "    user_query,\n",
        "    collection,\n",
        "    top_n=5,\n",
        "    vector_search_index_name=\"vector_search_index_scalar\",\n",
        "    text_search_index_name=\"text_search_index\",\n",
        "    vector_weight=0.7,\n",
        "    text_weight=0.3,\n",
        "):\n",
        "    \"\"\"\n",
        "    Perform hybrid search using both vector and text search with MongoDB RankFusion.\n",
        "\n",
        "    Args:\n",
        "        user_query (str): The user's query or search term.\n",
        "        collection (Collection): MongoDB collection object.\n",
        "        top_n (int): Number of results to return.\n",
        "        vector_search_index_name (str): Name of the vector search index.\n",
        "        text_search_index_name (str): Name of the text search index.\n",
        "        vector_weight (float): Weight for vector search results (0.0-1.0).\n",
        "        text_weight (float): Weight for text search results (0.0-1.0).\n",
        "\n",
        "    Returns:\n",
        "        List[Dict]: List of search results with scores and details.\n",
        "    \"\"\"\n",
        "\n",
        "    # Convert user query to embedding for vector search\n",
        "    query_embedding = get_voyage_embedding(user_query, input_type=\"query\")\n",
        "\n",
        "    # Build the RankFusion aggregation pipeline\n",
        "    rank_fusion_stage = {\n",
        "        \"$rankFusion\": {\n",
        "            \"input\": {\n",
        "                \"pipelines\": {\n",
        "                    \"vectorPipeline\": [\n",
        "                        {\n",
        "                            \"$vectorSearch\": {\n",
        "                                \"index\": vector_search_index_name,\n",
        "                                \"path\": \"embedding\",\n",
        "                                \"queryVector\": query_embedding,\n",
        "                                \"numCandidates\": 100,\n",
        "                                \"limit\": 20,\n",
        "                            }\n",
        "                        }\n",
        "                    ],\n",
        "                    \"textPipeline\": [\n",
        "                        {\n",
        "                            \"$search\": {\n",
        "                                \"index\": text_search_index_name,\n",
        "                                \"phrase\": {\n",
        "                                    \"query\": user_query,\n",
        "                                    \"path\": \"frame_description\",\n",
        "                                },\n",
        "                            }\n",
        "                        },\n",
        "                        {\"$limit\": 20},\n",
        "                    ],\n",
        "                }\n",
        "            },\n",
        "            \"combination\": {\n",
        "                \"weights\": {\n",
        "                    \"vectorPipeline\": vector_weight,\n",
        "                    \"textPipeline\": text_weight,\n",
        "                }\n",
        "            },\n",
        "            \"scoreDetails\": True,\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Project stage to select desired fields and include score details\n",
        "    project_stage = {\n",
        "        \"$project\": {\n",
        "            \"_id\": 0,\n",
        "            \"embedding\": 0,\n",
        "            \"scoreDetails\": {\"$meta\": \"scoreDetails\"},\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Final limit stage\n",
        "    limit_stage = {\"$limit\": top_n}\n",
        "\n",
        "    # Combine all stages into the complete aggregation pipeline\n",
        "    pipeline = [rank_fusion_stage, project_stage, limit_stage]\n",
        "\n",
        "    try:\n",
        "        # Execute the pipeline against the collection\n",
        "        results = list(collection.aggregate(pipeline))\n",
        "\n",
        "        print(f\"Found {len(results)} results for query: '{user_query}'\")\n",
        "\n",
        "        return results\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error executing hybrid search: {e}\")\n",
        "        return []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "scalar_hybrid_search_results = hybrid_search(\n",
        "    user_query=user_query,\n",
        "    collection=db[FRAME_INTELLIGENCE_METADATA],\n",
        "    top_n=5,\n",
        "    vector_search_index_name=\"vector_search_index_scalar\",\n",
        "    text_search_index_name=\"text_search_index\",\n",
        "    vector_weight=0.5,\n",
        "    text_weight=0.5,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "scalar_hybrid_search_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.3 Viewing the video player and returned time stamp\n",
        "\n",
        "This code creates an interactive video player for Jupyter notebooks that enables intelligent scene navigation based on AI search results. The `create_video_player_with_scenes()` function takes a video file and search results (containing timestamps, descriptions, and similarity scores), then generates an HTML interface with an embedded video player. It automatically handles video encoding by converting smaller files (under 50MB) to base64 for direct embedding, while serving larger videos from their local path with appropriate MIME type detection.\n",
        "\n",
        "The interface features a standard HTML5 video player with custom scene navigation controls below it. Users can click timestamp buttons to instantly jump to specific scenes, with each button showing the frame number, timestamp, similarity score, and description preview. When selected, the full scene description appears above the player, the video jumps to that timestamp, and playback begins automatically. The system includes keyboard shortcuts (spacebar for play/pause, arrow keys for 10-second navigation) and visual feedback effects, creating a seamless experience for exploring video content based on AI-generated embeddings and making it ideal for video analysis and content search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Dict, List\n",
        "\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "\n",
        "def create_video_player_with_scenes(\n",
        "    video_path: str,\n",
        "    search_results: List[Dict],\n",
        "    user_query: str = \"\",\n",
        "    width: int = 800,\n",
        "    height: int = 450,\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Create an interactive video player with scene navigation for Jupyter notebooks.\n",
        "\n",
        "    Args:\n",
        "        video_path (str): Path to the video file\n",
        "        search_results (List[Dict]): Search results with timestamps and descriptions\n",
        "        user_query (str): The original search query that generated these results\n",
        "        width (int): Video player width in pixels\n",
        "        height (int): Video player height in pixels\n",
        "    \"\"\"\n",
        "\n",
        "    if not os.path.exists(video_path):\n",
        "        print(f\"❌ Video file not found: {video_path}\")\n",
        "        return\n",
        "\n",
        "    if not search_results:\n",
        "        print(\"❌ No search results provided\")\n",
        "        return\n",
        "\n",
        "    # Convert video to base64 for embedding (for small videos)\n",
        "    # For large videos, you might want to serve via a local server\n",
        "    video_base64 = None\n",
        "    file_size_mb = os.path.getsize(video_path) / (1024 * 1024)\n",
        "\n",
        "    if file_size_mb < 50:  # Only embed videos smaller than 50MB\n",
        "        with open(video_path, \"rb\") as video_file:\n",
        "            video_data = video_file.read()\n",
        "            video_base64 = base64.b64encode(video_data).decode()\n",
        "\n",
        "    # Get video file extension for MIME type\n",
        "    file_ext = os.path.splitext(video_path)[1].lower()\n",
        "    mime_types = {\n",
        "        \".mp4\": \"video/mp4\",\n",
        "        \".webm\": \"video/webm\",\n",
        "        \".ogg\": \"video/ogg\",\n",
        "        \".avi\": \"video/mp4\",  # Fallback\n",
        "        \".mov\": \"video/mp4\",  # Fallback\n",
        "    }\n",
        "    video_mime = mime_types.get(file_ext, \"video/mp4\")\n",
        "\n",
        "    # Sort search results by timestamp\n",
        "    sorted_results = sorted(search_results, key=lambda x: x.get(\"frame_timestamp\", 0))\n",
        "\n",
        "    # Create HTML with embedded video player and controls\n",
        "    html_content = f\"\"\"\n",
        "    <div style=\"font-family: Arial, sans-serif; max-width: {width + 50}px;\">\n",
        "        <h3>🎬 Video Scene Navigator</h3>\n",
        "        \n",
        "        <!-- Search Query Display -->\n",
        "        {f'''<div style=\"background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 15px; border-radius: 8px; margin-bottom: 20px; box-shadow: 0 2px 10px rgba(0,0,0,0.1);\">\n",
        "            <div style=\"display: flex; align-items: center; gap: 10px;\">\n",
        "                <span style=\"font-size: 18px;\">🔍</span>\n",
        "                <div>\n",
        "                    <div style=\"font-size: 14px; opacity: 0.9; margin-bottom: 5px;\">Search Query:</div>\n",
        "                    <div style=\"font-size: 16px; font-weight: bold;\">\"{user_query}\"</div>\n",
        "                    <div style=\"font-size: 12px; opacity: 0.8; margin-top: 5px;\">Found {len(sorted_results)} matching scenes</div>\n",
        "                </div>\n",
        "            </div>\n",
        "        </div>''' if user_query else ''}\n",
        "        \n",
        "        <!-- Video Player -->\n",
        "        <div style=\"margin-bottom: 20px; border: 2px solid #ddd; border-radius: 8px; overflow: hidden;\">\n",
        "            <video id=\"videoPlayer\" width=\"{width}\" height=\"{height}\" controls style=\"display: block;\">\n",
        "                {\"<source src='data:\" + video_mime + \";base64,\" + video_base64 + \"' type='\" + video_mime + \"'>\" if video_base64 else \"<source src='\" + video_path + \"' type='\" + video_mime + \"'>\" }\n",
        "                Your browser does not support the video tag.\n",
        "            </video>\n",
        "        </div>\n",
        "        \n",
        "        <!-- Current Scene Info -->\n",
        "        <div id=\"currentScene\" style=\"background: #f0f8ff; padding: 15px; border-radius: 8px; margin-bottom: 20px; border-left: 4px solid #4CAF50;\">\n",
        "            <h4 style=\"margin: 0 0 10px 0; color: #333;\">📍 Current Scene</h4>\n",
        "            <p id=\"sceneDescription\" style=\"margin: 0; color: #666; font-style: italic;\">Click a timestamp below to view scene details</p>\n",
        "        </div>\n",
        "        \n",
        "        <!-- Scene Navigation Buttons -->\n",
        "        <div style=\"background: #f9f9f9; padding: 20px; border-radius: 8px;\">\n",
        "            <h4 style=\"margin: 0 0 15px 0; color: #333;\">🎯 Jump to Scenes</h4>\n",
        "            <div style=\"display: grid; grid-template-columns: repeat(auto-fit, minmax(250px, 1fr)); gap: 10px;\">\n",
        "\"\"\"\n",
        "\n",
        "    # Add scene buttons\n",
        "    for i, result in enumerate(sorted_results):\n",
        "        timestamp = result.get(\"frame_timestamp\", 0)\n",
        "        description = result.get(\"frame_description\", \"No description\")\n",
        "        score = result.get(\"score\", 0)\n",
        "        frame_number = result.get(\"frame_number\", 0)\n",
        "\n",
        "        # Truncate description for button\n",
        "        short_desc = description[:60] + \"...\" if len(description) > 60 else description\n",
        "\n",
        "        html_content += f\"\"\"\n",
        "                <button onclick=\"jumpToScene({timestamp}, `{description.replace('`', \"'\").replace('\"', \"'\")}`, {score}, {frame_number})\" \n",
        "                        style=\"padding: 10px; border: 1px solid #ddd; border-radius: 5px; background: white; cursor: pointer; text-align: left; transition: all 0.3s;\"\n",
        "                        onmouseover=\"this.style.background='#e3f2fd'; this.style.borderColor='#2196F3';\"\n",
        "                        onmouseout=\"this.style.background='white'; this.style.borderColor='#ddd';\">\n",
        "                    <div style=\"font-weight: bold; color: #1976D2; margin-bottom: 5px;\">\n",
        "                        ⏱️ {timestamp}s (Frame {frame_number}) | Score: {score:.3f}\n",
        "                    </div>\n",
        "                    <div style=\"font-size: 12px; color: #666;\">\n",
        "                        {short_desc}\n",
        "                    </div>\n",
        "                </button>\n",
        "\"\"\"\n",
        "\n",
        "    # Add JavaScript functionality\n",
        "    html_content += \"\"\"\n",
        "            </div>\n",
        "        </div>\n",
        "        \n",
        "        <!-- Video Controls Info -->\n",
        "        <div style=\"margin-top: 20px; padding: 15px; background: #fff3cd; border-radius: 8px; border-left: 4px solid #ffc107;\">\n",
        "            <h4 style=\"margin: 0 0 10px 0; color: #856404;\">💡 How to Use</h4>\n",
        "            <ul style=\"margin: 0; color: #856404; font-size: 14px;\">\n",
        "                <li>Click any timestamp button to jump to that scene</li>\n",
        "                <li>Use video controls to play, pause, and adjust volume</li>\n",
        "                <li>Scene descriptions appear above when you select a timestamp</li>\n",
        "            </ul>\n",
        "        </div>\n",
        "    </div>\n",
        "\n",
        "    <script>\n",
        "        function jumpToScene(timestamp, description, score, frameNumber) {\n",
        "            const video = document.getElementById('videoPlayer');\n",
        "            const sceneDesc = document.getElementById('sceneDescription');\n",
        "            \n",
        "            // Jump to timestamp\n",
        "            video.currentTime = timestamp;\n",
        "            \n",
        "            // Update scene description\n",
        "            sceneDesc.innerHTML = `\n",
        "                <div style=\"margin-bottom: 10px;\">\n",
        "                    <strong>🎬 Frame ${frameNumber} at ${timestamp}s (Score: ${score.toFixed(3)})</strong>\n",
        "                </div>\n",
        "                <div style=\"line-height: 1.5;\">\n",
        "                    ${description}\n",
        "                </div>\n",
        "            `;\n",
        "            \n",
        "            // Auto-play if paused\n",
        "            if (video.paused) {\n",
        "                video.play().catch(e => console.log('Auto-play prevented by browser'));\n",
        "            }\n",
        "            \n",
        "            // Scroll video into view\n",
        "            video.scrollIntoView({ behavior: 'smooth', block: 'nearest' });\n",
        "        }\n",
        "        \n",
        "        // Add time update listener to show current time\n",
        "        document.getElementById('videoPlayer').addEventListener('timeupdate', function() {\n",
        "            const currentTime = this.currentTime;\n",
        "            // You could add real-time scene detection here\n",
        "        });\n",
        "        \n",
        "        // Add keyboard shortcuts\n",
        "        document.addEventListener('keydown', function(e) {\n",
        "            const video = document.getElementById('videoPlayer');\n",
        "            \n",
        "            switch(e.key) {\n",
        "                case ' ':  // Spacebar to play/pause\n",
        "                    e.preventDefault();\n",
        "                    if (video.paused) {\n",
        "                        video.play();\n",
        "                    } else {\n",
        "                        video.pause();\n",
        "                    }\n",
        "                    break;\n",
        "                case 'ArrowLeft':  // Left arrow to go back 10s\n",
        "                    e.preventDefault();\n",
        "                    video.currentTime = Math.max(0, video.currentTime - 10);\n",
        "                    break;\n",
        "                case 'ArrowRight':  // Right arrow to go forward 10s\n",
        "                    e.preventDefault();\n",
        "                    video.currentTime = Math.min(video.duration, video.currentTime + 10);\n",
        "                    break;\n",
        "            }\n",
        "        });\n",
        "    </script>\n",
        "    \"\"\"\n",
        "\n",
        "    # Display the HTML\n",
        "    display(HTML(html_content))\n",
        "\n",
        "    print(f\"🎬 Video player created with {len(sorted_results)} scenes\")\n",
        "    print(f\"📁 Video: {os.path.basename(video_path)}\")\n",
        "    if file_size_mb >= 50:\n",
        "        print(\"⚠️  Large video file - serving from local path\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example usage\n",
        "video_path = \"videos/video.mp4\"  # Replace with your actual video path\n",
        "\n",
        "print(\"🎬 Creating interactive video player...\")\n",
        "create_video_player_with_scenes(\n",
        "    video_path,\n",
        "    full_fidelity_results,\n",
        "    user_query=\"Get me a scene where a player is injured or on the ground\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Making Things Agentic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "! pip install -Uq openai-agents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [],
      "source": [
        "OPENAI_MODEL = \"gpt-4o\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.1 Creating tools for our agents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "from agents.tool import function_tool\n",
        "\n",
        "\n",
        "@function_tool\n",
        "def get_frames_from_scene_description(scene_description: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Get frames from a scene description provided by the user\n",
        "\n",
        "    Args:\n",
        "        scene_description (str): The scene description to search for\n",
        "\n",
        "    Returns:\n",
        "        List[str]: A list of frame numbers that are relevant to the scene\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"Getting frames from scene description: {scene_description}\")\n",
        "\n",
        "    # Call the hybrid search function\n",
        "    results = hybrid_search(\n",
        "        user_query=scene_description,\n",
        "        collection=db[FRAME_INTELLIGENCE_METADATA],\n",
        "        top_n=5,\n",
        "        vector_search_index_name=\"vector_search_index_scalar\",\n",
        "        text_search_index_name=\"text_search_index\",\n",
        "        vector_weight=0.5,\n",
        "        text_weight=0.5,\n",
        "    )\n",
        "\n",
        "    print(f\"Frames found: {len(results)} for scene description: {scene_description}\")\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [],
      "source": [
        "@function_tool\n",
        "def get_frames_from_scene_image(scene_image: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Get frames from a scene image provided by the user\n",
        "\n",
        "    Args:\n",
        "        scene_description (str): The scene description to search for\n",
        "\n",
        "    Returns:\n",
        "        List[str]: A list of frame numbers that are relevant to the scene\n",
        "    \"\"\"\n",
        "\n",
        "    # Call the hybrid search function\n",
        "    results = hybrid_search(\n",
        "        user_query=scene_image,\n",
        "        collection=db[FRAME_INTELLIGENCE_METADATA],\n",
        "        top_n=5,\n",
        "        vector_search_index_name=\"vector_search_index_scalar\",\n",
        "        text_search_index_name=\"text_search_index\",\n",
        "        vector_weight=0.5,\n",
        "        text_weight=0.5,\n",
        "    )\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import List\n",
        "\n",
        "from pydantic import BaseModel\n",
        "\n",
        "\n",
        "# Define the FrameData class\n",
        "class FrameData(BaseModel):\n",
        "    frame_description: str\n",
        "    frame_number: int\n",
        "    frame_timestamp: float\n",
        "    score: float\n",
        "    filename: str = \"\"\n",
        "    embedding: List[float] = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Next agent displays the video player with the frames that are relevant to the scene description\n",
        "\n",
        "\n",
        "@function_tool\n",
        "def show_video_player_with_frames(frames: List[FrameData], user_query: str) -> str:\n",
        "    \"\"\"\n",
        "    Show the video player with the frame results from the frames intelligence metadata collection.\n",
        "\n",
        "    Args:\n",
        "        frames (List[FrameData]): A list of frame data objects that are relevant to the scene\n",
        "        user_query (str): The user query that triggered the intial search and is used to create the video player\n",
        "\n",
        "    Returns:\n",
        "        str: Success message\n",
        "    \"\"\"\n",
        "    # Convert Pydantic models to dictionaries\n",
        "    frames_dict = [frame.model_dump() for frame in frames]\n",
        "\n",
        "    # TODO: this needs to come from the frames intelligence metadata collection\n",
        "    video_path = \"videos/video.mp4\"  # Replace with your actual video path\n",
        "\n",
        "    print(f\"🎬 Creating interactive video player... with query: {user_query}\")\n",
        "    create_video_player_with_scenes(video_path, frames_dict, user_query=user_query)\n",
        "\n",
        "    return f\"Video player created with {len(frames)} relevant scenes for query: '{user_query}'\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [],
      "source": [
        "from agents import Agent, Runner\n",
        "\n",
        "frame_from_description_agent = Agent(\n",
        "    name=\"Frame From Descripton Retrieval Agent \",\n",
        "    instructions=\"You provide detailed information about frames from a video based on a scene description provided by the user. Always cite your sources.\",\n",
        "    handoff_description=\"A frame retrieval specialist that takes in a scene description that is a string provided by the user and returns a list of frames that are relevant to the scene\",\n",
        "    tools=[get_frames_from_scene_description],\n",
        ")\n",
        "\n",
        "frame_from_scene_image_agent = Agent(\n",
        "    name=\"Frame From Scene Image Retrieval Agent\",\n",
        "    instructions=\"You provide detailed information about frames from a video based on a scene image provided by the user. Always cite your sources.\",\n",
        "    handoff_description=\"A frame retrieval specialist that takes in a scene image that is a string provided by the user and returns a list of frames that are relevant to the scene\",\n",
        "    tools=[get_frames_from_scene_image],\n",
        ")\n",
        "\n",
        "show_video_player_agent = Agent(\n",
        "    name=\"Show Video Player Agent\",\n",
        "    instructions=\"You display the video player with the frames that are relevant to the scene description\",\n",
        "    handoff_description=\"A video player specialist that takes in a list of frames that are relevant to the scene and displays them in a video player\",\n",
        "    tools=[show_video_player_with_frames],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create an orchestrator agent that can use both specialized agents as tools\n",
        "orchestrator_agent = Agent(\n",
        "    name=\"video_intelligence_orchestrator\",\n",
        "    instructions=(\n",
        "        \"You are a video intelligence assistant. Your job is to help users by retrieving relevant information using your tools.\\n\\n\"\n",
        "        \"IMPORTANT RULES:\\n\"\n",
        "        \"1. ALWAYS use frame_from_description_agent when a query mentions a scene description\\n\"\n",
        "        \"2. ALWAYS use frame_from_scene_image_agent when a query mentions a scene image\\n\"\n",
        "        \"3. If a query requires BOTH frame from description AND frame from scene image, use BOTH tools in sequence\\n\"\n",
        "        \"4. NEVER attempt to provide video intelligence information without using your tools\\n\"\n",
        "        \"5. Each tool provides different types of information - use all appropriate tools for complete assistance\"\n",
        "    ),\n",
        "    tools=[\n",
        "        frame_from_description_agent.as_tool(\n",
        "            tool_name=\"frame_from_description\",\n",
        "            tool_description=\"Get frames from a scene description\",\n",
        "        ),\n",
        "        frame_from_scene_image_agent.as_tool(\n",
        "            tool_name=\"frame_from_scene_image\",\n",
        "            tool_description=\"Get frames from a scene image\",\n",
        "        ),\n",
        "        show_video_player_agent.as_tool(\n",
        "            tool_name=\"show_video_player\",\n",
        "            tool_description=\"Show the video player with the frames that are relevant to the scene\",\n",
        "        ),\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [],
      "source": [
        "from agents import ItemHelpers, MessageOutputItem, trace\n",
        "\n",
        "\n",
        "async def video_intelligence_assistant(user_query):\n",
        "    \"\"\"Run the complete virtual primary care assistant workflow\"\"\"\n",
        "    # First, have the orchestrator determine which tools to use\n",
        "    with trace(\"Orchestrator evaluator\"):\n",
        "        orchestrator_result = await Runner.run(orchestrator_agent, user_query)\n",
        "\n",
        "        # Print intermediate steps for debugging/transparency\n",
        "        print(\"\\n--- Orchestrator Processing Steps ---\")\n",
        "        for item in orchestrator_result.new_items:\n",
        "            if isinstance(item, MessageOutputItem):\n",
        "                text = ItemHelpers.text_message_output(item)\n",
        "                if text:\n",
        "                    print(f\"  - Information gathering step: {text}\")\n",
        "\n",
        "        print(\n",
        "            f\"\\n\\n--- Final Video Intelligence Response ---\\n{orchestrator_result.final_output}\"\n",
        "        )\n",
        "        print()\n",
        "\n",
        "    return orchestrator_result.final_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [],
      "source": [
        "import asyncio\n",
        "\n",
        "import nest_asyncio\n",
        "\n",
        "# Apply nest_asyncio to patch the event loop\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_video_intelligence_assistant(query):\n",
        "    # Create a new event loop\n",
        "    loop = asyncio.new_event_loop()\n",
        "    asyncio.set_event_loop(loop)\n",
        "\n",
        "    # Run the async function and get the result\n",
        "    result = loop.run_until_complete(video_intelligence_assistant(query))\n",
        "\n",
        "    # Clean up\n",
        "    loop.close()\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Now call the function this way\n",
        "query = input(\"What video intelligence can I help you with today? \")\n",
        "run_video_intelligence_assistant(query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Live Stream Diagonsis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [],
      "source": [
        "import threading\n",
        "import time\n",
        "from typing import Optional\n",
        "\n",
        "import yt_dlp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [],
      "source": [
        "def search_similar_incidents(embedding, collection, threshold=0.7):\n",
        "    \"\"\"Search for similar incidents in database\"\"\"\n",
        "    try:\n",
        "        # Simplified vector search\n",
        "        pipeline = [\n",
        "            {\n",
        "                \"$vectorSearch\": {\n",
        "                    \"index\": \"incident_vector_index_scalar\",\n",
        "                    \"path\": \"embedding\",\n",
        "                    \"queryVector\": embedding,\n",
        "                    \"numCandidates\": 100,\n",
        "                    \"limit\": 10,\n",
        "                }\n",
        "            },\n",
        "            {\"$addFields\": {\"similarity_score\": {\"$meta\": \"vectorSearchScore\"}}},\n",
        "            {\"$match\": {\"similarity_score\": {\"$gte\": threshold}}},\n",
        "        ]\n",
        "\n",
        "        results = list(collection.aggregate(pipeline))\n",
        "        return results\n",
        "    except Exception as e:\n",
        "        print(f\"Error searching incidents: {e}\")\n",
        "        return []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Global variables for the monitoring system\n",
        "current_monitoring = {\n",
        "    \"active\": False,\n",
        "    \"cap\": None,\n",
        "    \"frame_count\": 0,\n",
        "    \"incidents\": [],\n",
        "    \"stats\": {\"frames_processed\": 0, \"incidents_detected\": 0},\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# HELPER FUNCTIONS\n",
        "# =============================================================================\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "def _monitoring_worker():\n",
        "    \"\"\"Background worker for processing frames\"\"\"\n",
        "    global current_monitoring\n",
        "\n",
        "    # Connect to MongoDB\n",
        "    try:\n",
        "        collection = db[PREVIOUS_FRAME_INCIDENTS]\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Could not connect to MongoDB: {e}\")\n",
        "        collection = None\n",
        "\n",
        "    while current_monitoring[\"active\"]:\n",
        "        try:\n",
        "            if not current_monitoring[\"cap\"]:\n",
        "                break\n",
        "\n",
        "            ret, frame = current_monitoring[\"cap\"].read()\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            current_monitoring[\"frame_count\"] += 1\n",
        "\n",
        "            # Process every N frames\n",
        "            if (\n",
        "                current_monitoring[\"frame_count\"]\n",
        "                % current_monitoring.get(\"check_interval\", 30)\n",
        "                == 0\n",
        "            ):\n",
        "                _process_frame_for_incidents(frame, collection)\n",
        "\n",
        "            time.sleep(0.01)  # Small delay\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Monitoring error: {e}\")\n",
        "            break\n",
        "\n",
        "\n",
        "def _process_frame_for_incidents(frame, collection):\n",
        "    \"\"\"Process a single frame for incident detection\"\"\"\n",
        "    try:\n",
        "        # Convert frame to embedding\n",
        "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        pil_image = Image.fromarray(rgb_frame)\n",
        "        embedding = get_voyage_embedding(pil_image, \"query\")\n",
        "\n",
        "        # Search for similar incidents\n",
        "        if collection is not None:\n",
        "            incidents = search_similar_incidents(embedding, collection)\n",
        "\n",
        "            for incident in incidents:\n",
        "                incident_data = {\n",
        "                    \"incident_type\": incident.get(\"incident_type\", \"Unknown\"),\n",
        "                    \"network_issue\": incident.get(\"network_issue\", \"Unknown\"),\n",
        "                    \"resolution_text\": incident.get(\"resolution_text\", \"No solution\"),\n",
        "                    \"similarity_score\": incident.get(\"similarity_score\", 0),\n",
        "                    \"frame_number\": current_monitoring[\"frame_count\"],\n",
        "                    \"timestamp\": datetime.now().isoformat(),\n",
        "                }\n",
        "\n",
        "                current_monitoring[\"incidents\"].append(incident_data)\n",
        "                current_monitoring[\"stats\"][\"incidents_detected\"] += 1\n",
        "\n",
        "        current_monitoring[\"stats\"][\"frames_processed\"] += 1\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Frame processing error: {e}\")\n",
        "\n",
        "\n",
        "def _add_incident_overlays(frame):\n",
        "    \"\"\"Add incident detection overlays to frame\"\"\"\n",
        "    # Get recent incidents (last 10 seconds worth)\n",
        "    recent_incidents = current_monitoring[\"incidents\"][-3:]  # Show last 3\n",
        "\n",
        "    if not recent_incidents:\n",
        "        return frame\n",
        "\n",
        "    # Add overlay background\n",
        "    overlay_y = 30\n",
        "    for incident in recent_incidents:\n",
        "        # Create text\n",
        "        text_lines = [\n",
        "            f\"⚠️ {incident['incident_type']} (Score: {incident['similarity_score']:.2f})\",\n",
        "            f\"Issue: {incident['network_issue'][:40]}...\",\n",
        "            f\"Solution: {incident['resolution_text'][:40]}...\",\n",
        "        ]\n",
        "\n",
        "        # Draw background\n",
        "        max_width = max(len(line) for line in text_lines) * 8\n",
        "        rect_height = len(text_lines) * 20 + 10\n",
        "\n",
        "        cv2.rectangle(\n",
        "            frame, (10, overlay_y), (max_width, overlay_y + rect_height), (0, 0, 0), -1\n",
        "        )\n",
        "        cv2.rectangle(\n",
        "            frame,\n",
        "            (10, overlay_y),\n",
        "            (max_width, overlay_y + rect_height),\n",
        "            (0, 255, 255),\n",
        "            2,\n",
        "        )\n",
        "\n",
        "        # Draw text\n",
        "        for i, line in enumerate(text_lines):\n",
        "            y_pos = overlay_y + 15 + (i * 20)\n",
        "            cv2.putText(\n",
        "                frame,\n",
        "                line,\n",
        "                (15, y_pos),\n",
        "                cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                0.5,\n",
        "                (255, 255, 255),\n",
        "                1,\n",
        "            )\n",
        "\n",
        "        overlay_y += rect_height + 10\n",
        "\n",
        "    # Add stats\n",
        "    stats_text = f\"Frames: {current_monitoring['stats']['frames_processed']} | Incidents: {current_monitoring['stats']['incidents_detected']}\"\n",
        "    cv2.putText(\n",
        "        frame,\n",
        "        stats_text,\n",
        "        (10, frame.shape[0] - 20),\n",
        "        cv2.FONT_HERSHEY_SIMPLEX,\n",
        "        0.5,\n",
        "        (0, 255, 0),\n",
        "        1,\n",
        "    )\n",
        "\n",
        "    return frame\n",
        "\n",
        "\n",
        "def _print_stats():\n",
        "    \"\"\"Print current statistics\"\"\"\n",
        "    stats = current_monitoring[\"stats\"]\n",
        "    print(\n",
        "        f\"\\n📊 Stats: Frames: {stats['frames_processed']} | Incidents: {stats['incidents_detected']}\"\n",
        "    )\n",
        "    print(f\"Current frame: {current_monitoring['frame_count']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_youtube_url(youtube_url: str) -> Optional[str]:\n",
        "    \"\"\"Extract direct stream URL from YouTube\"\"\"\n",
        "    try:\n",
        "        print(f\"🔍 Extracting stream from: {youtube_url}\")\n",
        "\n",
        "        ydl_opts = {\n",
        "            \"format\": \"best[height<=720]/best\",  # Fallback to any quality\n",
        "            \"quiet\": True,\n",
        "            \"no_warnings\": True,\n",
        "            \"extract_flat\": False,\n",
        "        }\n",
        "\n",
        "        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "            print(\"📥 Downloading video info...\")\n",
        "            info = ydl.extract_info(youtube_url, download=False)\n",
        "\n",
        "            if not info:\n",
        "                print(\"❌ No video information extracted\")\n",
        "                return None\n",
        "\n",
        "            print(f\"📺 Video title: {info.get('title', 'Unknown')}\")\n",
        "            print(f\"📺 Uploader: {info.get('uploader', 'Unknown')}\")\n",
        "            print(f\"🔴 Is live: {info.get('is_live', False)}\")\n",
        "            print(f\"⏱️ Duration: {info.get('duration', 'Unknown')}\")\n",
        "\n",
        "            # Get the best format URL\n",
        "            formats = info.get(\"formats\", [])\n",
        "            print(f\"📊 Found {len(formats)} formats\")\n",
        "\n",
        "            if not formats:\n",
        "                print(\"❌ No video formats available\")\n",
        "                return None\n",
        "\n",
        "            # Try to find a good format\n",
        "            best_format = None\n",
        "            for fmt in formats:\n",
        "                if fmt.get(\"vcodec\") != \"none\" and fmt.get(\"url\"):\n",
        "                    if not best_format or (\n",
        "                        fmt.get(\"height\", 0) > best_format.get(\"height\", 0)\n",
        "                    ):\n",
        "                        best_format = fmt\n",
        "                        print(\n",
        "                            f\"📊 Found format: {fmt.get('format_id')} - {fmt.get('width')}x{fmt.get('height')}\"\n",
        "                        )\n",
        "\n",
        "            if not best_format:\n",
        "                print(\"❌ No suitable video format found\")\n",
        "                return None\n",
        "\n",
        "            stream_url = best_format[\"url\"]\n",
        "            print(f\"✅ Stream URL extracted: {stream_url[:60]}...\")\n",
        "            return stream_url\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"💥 YouTube extraction error: {type(e).__name__}: {e}\")\n",
        "\n",
        "        # Common YouTube errors and solutions\n",
        "        if \"Video unavailable\" in str(e):\n",
        "            print(\"💡 Video may be private, deleted, or geo-blocked\")\n",
        "        elif \"Sign in to confirm your age\" in str(e):\n",
        "            print(\"💡 Video requires age verification\")\n",
        "        elif \"This live event has ended\" in str(e):\n",
        "            print(\"💡 Livestream has ended\")\n",
        "        else:\n",
        "            print(\"💡 Try a different YouTube video or check if the URL is correct\")\n",
        "\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [],
      "source": [
        "@function_tool\n",
        "def stop_video_monitoring() -> str:\n",
        "    \"\"\"\n",
        "    Stop current video monitoring.\n",
        "\n",
        "    Returns:\n",
        "        Status message with final statistics\n",
        "    \"\"\"\n",
        "    global current_monitoring\n",
        "\n",
        "    if not current_monitoring[\"active\"]:\n",
        "        return \"No active monitoring to stop\"\n",
        "\n",
        "    # Stop monitoring\n",
        "    current_monitoring[\"active\"] = False\n",
        "\n",
        "    if current_monitoring[\"cap\"]:\n",
        "        current_monitoring[\"cap\"].release()\n",
        "\n",
        "    # Get final stats\n",
        "    final_stats = {\n",
        "        \"total_frames\": current_monitoring[\"frame_count\"],\n",
        "        \"frames_processed\": current_monitoring[\"stats\"][\"frames_processed\"],\n",
        "        \"incidents_detected\": current_monitoring[\"stats\"][\"incidents_detected\"],\n",
        "    }\n",
        "\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "    return f\"🛑 Monitoring stopped. Final stats: {final_stats}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {},
      "outputs": [],
      "source": [
        "@function_tool\n",
        "def start_video_monitoring(source: str = \"0\", check_interval: int = 30) -> str:\n",
        "    \"\"\"\n",
        "    Start real-time video monitoring for incident detection.\n",
        "\n",
        "    Args:\n",
        "        source: Video source (0 for webcam, file path, or YouTube URL)\n",
        "        check_interval: Check every N frames\n",
        "\n",
        "    Returns:\n",
        "        Status message\n",
        "    \"\"\"\n",
        "    global current_monitoring\n",
        "\n",
        "    try:\n",
        "        # Stop existing monitoring\n",
        "        if current_monitoring[\"active\"]:\n",
        "            stop_video_monitoring()\n",
        "\n",
        "        print(f\"🔍 Processing source: {source}\")\n",
        "\n",
        "        # Handle different source types\n",
        "        if source.startswith(\"http\") and \"youtube\" in source:\n",
        "            print(\"📺 Detected YouTube URL, extracting stream...\")\n",
        "            stream_url = extract_youtube_url(source)\n",
        "            if not stream_url:\n",
        "                return f\"❌ Failed to extract YouTube stream URL from: {source}. Video may be private, restricted, or unavailable.\"\n",
        "            print(f\"✅ Extracted stream URL: {stream_url[:60]}...\")\n",
        "            video_source = stream_url\n",
        "        elif source.isdigit():\n",
        "            video_source = int(source)  # Webcam\n",
        "            print(f\"📷 Using webcam: {video_source}\")\n",
        "        else:\n",
        "            video_source = source  # File path\n",
        "            print(f\"📁 Using file: {video_source}\")\n",
        "\n",
        "        # Start video capture with detailed error checking\n",
        "        print(\"🎥 Initializing video capture...\")\n",
        "        cap = cv2.VideoCapture(video_source)\n",
        "\n",
        "        if not cap.isOpened():\n",
        "            error_msg = f\"❌ Could not open video source: {source}\"\n",
        "            if isinstance(video_source, str) and video_source.startswith(\"http\"):\n",
        "                error_msg += \"\\n   - YouTube stream URL may be invalid or expired\"\n",
        "                error_msg += (\n",
        "                    \"\\n   - Try a different video or check if the stream is live\"\n",
        "                )\n",
        "            print(error_msg)\n",
        "            return error_msg\n",
        "\n",
        "        # Test if we can actually read a frame\n",
        "        print(\"🧪 Testing frame capture...\")\n",
        "        ret, test_frame = cap.read()\n",
        "        if not ret or test_frame is None:\n",
        "            cap.release()\n",
        "            error_msg = f\"❌ Could not read frames from video source: {source}\"\n",
        "            if isinstance(video_source, str) and video_source.startswith(\"http\"):\n",
        "                error_msg += \"\\n   - Stream may have ended or be unavailable\"\n",
        "            print(error_msg)\n",
        "            return error_msg\n",
        "\n",
        "        print(f\"✅ Successfully captured test frame: {test_frame.shape}\")\n",
        "\n",
        "        # Initialize monitoring\n",
        "        current_monitoring[\"active\"] = True\n",
        "        current_monitoring[\"cap\"] = cap\n",
        "        current_monitoring[\"frame_count\"] = 0\n",
        "        current_monitoring[\"incidents\"] = []\n",
        "        current_monitoring[\"check_interval\"] = check_interval\n",
        "        current_monitoring[\"source\"] = source\n",
        "\n",
        "        # Start monitoring thread\n",
        "        monitor_thread = threading.Thread(target=_monitoring_worker, daemon=True)\n",
        "        monitor_thread.start()\n",
        "\n",
        "        success_msg = f\"✅ Started monitoring source: {source} (checking every {check_interval} frames)\"\n",
        "        print(success_msg)\n",
        "        return success_msg\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"❌ Error starting monitoring: {e}\"\n",
        "        print(f\"💥 Exception details: {type(e).__name__}: {e}\")\n",
        "        return error_msg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "\n",
        "@function_tool\n",
        "def get_monitoring_status() -> dict:\n",
        "    \"\"\"\n",
        "    Get current monitoring status and statistics.\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with monitoring status and stats\n",
        "    \"\"\"\n",
        "    global current_monitoring\n",
        "\n",
        "    return {\n",
        "        \"active\": current_monitoring[\"active\"],\n",
        "        \"frame_count\": current_monitoring[\"frame_count\"],\n",
        "        \"frames_processed\": current_monitoring[\"stats\"][\"frames_processed\"],\n",
        "        \"incidents_detected\": current_monitoring[\"stats\"][\"incidents_detected\"],\n",
        "        \"recent_incidents\": len(current_monitoring[\"incidents\"]),\n",
        "        \"timestamp\": datetime.now().isoformat(),\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {},
      "outputs": [],
      "source": [
        "@function_tool\n",
        "def get_recent_incidents(max_incidents: int = 5) -> list:\n",
        "    \"\"\"\n",
        "    Get recent incident detections.\n",
        "\n",
        "    Args:\n",
        "        max_incidents: Maximum number of incidents to return\n",
        "\n",
        "    Returns:\n",
        "        List of recent incidents\n",
        "    \"\"\"\n",
        "    global current_monitoring\n",
        "\n",
        "    recent = current_monitoring[\"incidents\"][-max_incidents:]\n",
        "\n",
        "    # Convert to simple format\n",
        "    incidents = []\n",
        "    for incident in recent:\n",
        "        incidents.append(\n",
        "            {\n",
        "                \"incident_type\": incident.get(\"incident_type\", \"Unknown\"),\n",
        "                \"network_issue\": incident.get(\"network_issue\", \"Unknown issue\"),\n",
        "                \"resolution_text\": incident.get(\n",
        "                    \"resolution_text\", \"No solution available\"\n",
        "                ),\n",
        "                \"similarity_score\": incident.get(\"similarity_score\", 0),\n",
        "                \"frame_number\": incident.get(\"frame_number\", 0),\n",
        "                \"timestamp\": incident.get(\"timestamp\", datetime.now().isoformat()),\n",
        "            }\n",
        "        )\n",
        "\n",
        "    return incidents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {},
      "outputs": [],
      "source": [
        "@function_tool\n",
        "def show_video_feed(show_overlays: bool = True) -> str:\n",
        "    \"\"\"\n",
        "    Display the video feed with optional incident overlays.\n",
        "\n",
        "    Args:\n",
        "        show_overlays: Whether to show incident detection overlays\n",
        "\n",
        "    Returns:\n",
        "        Status message\n",
        "    \"\"\"\n",
        "    global current_monitoring\n",
        "\n",
        "    if not current_monitoring[\"active\"]:\n",
        "        return \"❌ No active video monitoring to display. Start monitoring first with 'Start monitoring [source]'\"\n",
        "\n",
        "    if not current_monitoring[\"cap\"]:\n",
        "        return \"❌ Video capture not available. There may be an issue with the video source.\"\n",
        "\n",
        "    try:\n",
        "        print(\"🎥 Displaying video feed... (Press 'q' to quit, 's' for stats)\")\n",
        "        print(f\"📺 Source: {current_monitoring.get('source', 'Unknown')}\")\n",
        "        print(f\"🔄 Frame count: {current_monitoring['frame_count']}\")\n",
        "\n",
        "        display_count = 0\n",
        "        max_empty_frames = 10\n",
        "        empty_frame_count = 0\n",
        "\n",
        "        while current_monitoring[\"active\"]:\n",
        "            ret, frame = current_monitoring[\"cap\"].read()\n",
        "            if not ret:\n",
        "                empty_frame_count += 1\n",
        "                print(f\"⚠️ Failed to read frame {empty_frame_count}/{max_empty_frames}\")\n",
        "\n",
        "                if empty_frame_count >= max_empty_frames:\n",
        "                    print(\"❌ Too many failed frame reads, stopping display\")\n",
        "                    break\n",
        "\n",
        "                time.sleep(0.1)  # Wait a bit before trying again\n",
        "                continue\n",
        "\n",
        "            # Reset empty frame counter on successful read\n",
        "            empty_frame_count = 0\n",
        "            display_count += 1\n",
        "\n",
        "            # Add overlays if requested\n",
        "            if show_overlays:\n",
        "                frame = _add_incident_overlays(frame)\n",
        "\n",
        "            # Add display info\n",
        "            info_text = f\"Display: {display_count} | Source: {current_monitoring.get('source', 'Unknown')[:30]}\"\n",
        "            cv2.putText(\n",
        "                frame,\n",
        "                info_text,\n",
        "                (10, frame.shape[0] - 70),\n",
        "                cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                0.5,\n",
        "                (255, 255, 255),\n",
        "                1,\n",
        "            )\n",
        "\n",
        "            # Display frame\n",
        "            cv2.imshow(\"Real-time Video Analysis\", frame)\n",
        "\n",
        "            # Handle key presses\n",
        "            key = cv2.waitKey(1) & 0xFF\n",
        "            if key == ord(\"q\"):\n",
        "                print(\"🛑 Display stopped by user\")\n",
        "                break\n",
        "            elif key == ord(\"s\"):\n",
        "                _print_stats()\n",
        "\n",
        "        cv2.destroyAllWindows()\n",
        "        return f\"✅ Video display closed after showing {display_count} frames\"\n",
        "\n",
        "    except Exception as e:\n",
        "        cv2.destroyAllWindows()\n",
        "        error_msg = f\"❌ Error displaying video: {e}\"\n",
        "        print(f\"💥 Display error: {type(e).__name__}: {e}\")\n",
        "        return error_msg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {},
      "outputs": [],
      "source": [
        "@function_tool\n",
        "def show_latest_frame() -> str:\n",
        "    \"\"\"\n",
        "    Capture and display the current frame in Jupyter notebook.\n",
        "\n",
        "    Returns:\n",
        "        Status message with frame info\n",
        "    \"\"\"\n",
        "    global current_monitoring\n",
        "\n",
        "    if not current_monitoring[\"active\"] or not current_monitoring[\"cap\"]:\n",
        "        return \"❌ No active video monitoring to capture frame from\"\n",
        "\n",
        "    try:\n",
        "        # Capture current frame\n",
        "        ret, frame = current_monitoring[\"cap\"].read()\n",
        "        if not ret:\n",
        "            return \"❌ Could not capture current frame\"\n",
        "\n",
        "        # Add overlays\n",
        "        frame = _add_incident_overlays(frame)\n",
        "\n",
        "        # Add info\n",
        "        info_text = f\"Frame: {current_monitoring['frame_count']} | Live capture\"\n",
        "        cv2.putText(\n",
        "            frame,\n",
        "            info_text,\n",
        "            (10, 30),\n",
        "            cv2.FONT_HERSHEY_SIMPLEX,\n",
        "            0.7,\n",
        "            (255, 255, 255),\n",
        "            2,\n",
        "        )\n",
        "\n",
        "        # Save current frame\n",
        "        import os\n",
        "\n",
        "        os.makedirs(\"video_frames\", exist_ok=True)\n",
        "        filename = \"video_frames/current_frame.jpg\"\n",
        "        cv2.imwrite(filename, frame)\n",
        "\n",
        "        # Display in notebook using IPython\n",
        "        try:\n",
        "            from IPython.display import Image, display\n",
        "\n",
        "            display(Image(filename))\n",
        "            return f\"✅ Current frame displayed (Frame #{current_monitoring['frame_count']})\"\n",
        "        except ImportError:\n",
        "            return f\"✅ Current frame saved to {filename} (Frame #{current_monitoring['frame_count']})\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"❌ Error capturing frame: {e}\"\n",
        "\n",
        "\n",
        "@function_tool\n",
        "def create_video_summary() -> str:\n",
        "    \"\"\"\n",
        "    Create a summary of the current monitoring session.\n",
        "\n",
        "    Returns:\n",
        "        Summary of monitoring statistics and recent activity\n",
        "    \"\"\"\n",
        "    global current_monitoring\n",
        "\n",
        "    if not current_monitoring.get(\"active\", False):\n",
        "        return \"❌ No active monitoring session\"\n",
        "\n",
        "    try:\n",
        "        summary = f\"\"\"\n",
        "📊 **Video Monitoring Summary**\n",
        "\n",
        "🎬 **Source**: {current_monitoring.get('source', 'Unknown')}\n",
        "📊 **Status**: {'🟢 Active' if current_monitoring['active'] else '🔴 Stopped'}\n",
        "🔢 **Total Frames**: {current_monitoring.get('frame_count', 0)}\n",
        "⚙️ **Processed**: {current_monitoring.get('stats', {}).get('frames_processed', 0)}\n",
        "🚨 **Incidents**: {current_monitoring.get('stats', {}).get('incidents_detected', 0)}\n",
        "⏱️ **Check Interval**: Every {current_monitoring.get('check_interval', 'Unknown')} frames\n",
        "\n",
        "🎯 **Recent Incidents**: {len(current_monitoring.get('incidents', []))} detected\n",
        "        \"\"\"\n",
        "\n",
        "        # Add recent incidents if any\n",
        "        recent_incidents = current_monitoring.get(\"incidents\", [])[-3:]  # Last 3\n",
        "        if recent_incidents:\n",
        "            summary += \"\\n🚨 **Latest Incidents**:\\n\"\n",
        "            for i, incident in enumerate(recent_incidents, 1):\n",
        "                summary += f\"   {i}. {incident.get('incident_type', 'Unknown')} (Score: {incident.get('similarity_score', 0):.2f})\\n\"\n",
        "        else:\n",
        "            summary += \"\\n✅ **No Recent Incidents Detected**\"\n",
        "\n",
        "        return summary\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"❌ Error creating summary: {e}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
        "@function_tool\n",
        "def get_youtube_stream_info(youtube_url: str) -> dict:\n",
        "    \"\"\"\n",
        "    Get direct stream information from a YouTube URL.\n",
        "\n",
        "    Args:\n",
        "        youtube_url: YouTube video/livestream URL\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with stream information and direct URL\n",
        "    \"\"\"\n",
        "    try:\n",
        "        ydl_opts = {\n",
        "            \"format\": \"best[height<=720]\",\n",
        "            \"quiet\": True,\n",
        "            \"no_warnings\": True,\n",
        "            \"extractaudio\": False,\n",
        "        }\n",
        "\n",
        "        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "            info = ydl.extract_info(youtube_url, download=False)\n",
        "\n",
        "            if not info:\n",
        "                return {\"error\": \"Could not extract video information\"}\n",
        "\n",
        "            # Get the best format URL\n",
        "            formats = info.get(\"formats\", [])\n",
        "            best_format = None\n",
        "\n",
        "            for fmt in formats:\n",
        "                if fmt.get(\"vcodec\") != \"none\" and fmt.get(\"url\"):\n",
        "                    if not best_format or (\n",
        "                        fmt.get(\"height\", 0) > best_format.get(\"height\", 0)\n",
        "                    ):\n",
        "                        best_format = fmt\n",
        "\n",
        "            if not best_format:\n",
        "                return {\"error\": \"No suitable video format found\"}\n",
        "\n",
        "            return {\n",
        "                \"success\": True,\n",
        "                \"title\": info.get(\"title\", \"Unknown\"),\n",
        "                \"uploader\": info.get(\"uploader\", \"Unknown\"),\n",
        "                \"is_live\": info.get(\"is_live\", False),\n",
        "                \"duration\": info.get(\"duration\"),\n",
        "                \"direct_url\": best_format[\"url\"],\n",
        "                \"width\": best_format.get(\"width\"),\n",
        "                \"height\": best_format.get(\"height\"),\n",
        "                \"fps\": best_format.get(\"fps\"),\n",
        "                \"original_url\": youtube_url,\n",
        "            }\n",
        "\n",
        "    except Exception as e:\n",
        "        return {\"error\": f\"Failed to extract stream: {e}\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {},
      "outputs": [],
      "source": [
        "@function_tool\n",
        "def show_video_feed_jupyter(\n",
        "    show_overlays: bool = True, save_frames: bool = True\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Display video feed in Jupyter notebook (saves frames instead of using cv2.imshow).\n",
        "\n",
        "    Args:\n",
        "        show_overlays: Whether to show incident detection overlays\n",
        "        save_frames: Whether to save frames to files\n",
        "\n",
        "    Returns:\n",
        "        Status message\n",
        "    \"\"\"\n",
        "    global current_monitoring\n",
        "\n",
        "    if not current_monitoring[\"active\"]:\n",
        "        return \"❌ No active video monitoring to display. Start monitoring first.\"\n",
        "\n",
        "    if not current_monitoring[\"cap\"]:\n",
        "        return \"❌ Video capture not available.\"\n",
        "\n",
        "    try:\n",
        "        print(\"🎥 Jupyter-friendly video display starting...\")\n",
        "        print(f\"📺 Source: {current_monitoring.get('source', 'Unknown')}\")\n",
        "        print(f\"🔄 Current frame count: {current_monitoring['frame_count']}\")\n",
        "        print(\"📁 Frames will be saved to 'video_frames/' directory\")\n",
        "        print(\"🛑 Run 'Stop monitoring' to stop\")\n",
        "\n",
        "        # Create frames directory\n",
        "        import os\n",
        "\n",
        "        os.makedirs(\"video_frames\", exist_ok=True)\n",
        "\n",
        "        frame_save_count = 0\n",
        "        max_frames_to_save = 10  # Save every 30th frame for demo\n",
        "\n",
        "        for i in range(max_frames_to_save):\n",
        "            if not current_monitoring[\"active\"]:\n",
        "                break\n",
        "\n",
        "            ret, frame = current_monitoring[\"cap\"].read()\n",
        "            if not ret:\n",
        "                print(f\"⚠️ Could not read frame {i+1}\")\n",
        "                continue\n",
        "\n",
        "            # Add overlays if requested\n",
        "            if show_overlays:\n",
        "                frame = _add_incident_overlays(frame)\n",
        "\n",
        "            # Add frame info\n",
        "            info_text = f\"Frame: {current_monitoring['frame_count']} | {current_monitoring.get('source', 'Unknown')[:30]}\"\n",
        "            cv2.putText(\n",
        "                frame,\n",
        "                info_text,\n",
        "                (10, 30),\n",
        "                cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                0.7,\n",
        "                (255, 255, 255),\n",
        "                2,\n",
        "            )\n",
        "\n",
        "            # Save frame\n",
        "            if save_frames:\n",
        "                filename = f\"video_frames/frame_{frame_save_count:04d}.jpg\"\n",
        "                cv2.imwrite(filename, frame)\n",
        "                frame_save_count += 1\n",
        "                print(f\"💾 Saved frame {frame_save_count}: {filename}\")\n",
        "\n",
        "            # Small delay\n",
        "            import time\n",
        "\n",
        "            time.sleep(0.5)  # Save a frame every 0.5 seconds\n",
        "\n",
        "        return f\"✅ Saved {frame_save_count} frames to 'video_frames/' directory. Monitoring continues in background.\"\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"❌ Error in Jupyter display: {e}\"\n",
        "        print(f\"💥 Display error: {type(e).__name__}: {e}\")\n",
        "        return error_msg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# AGENTS\n",
        "# =============================================================================\n",
        "\n",
        "from agents import Agent, Runner\n",
        "\n",
        "# Simple monitoring agent\n",
        "monitoring_agent = Agent(\n",
        "    name=\"Video Monitoring Agent\",\n",
        "    instructions=\"You start and stop video monitoring for incident detection. You can handle webcam, files, and YouTube streams.\",\n",
        "    handoff_description=\"Specialist for starting and managing video stream monitoring\",\n",
        "    tools=[start_video_monitoring, stop_video_monitoring, get_monitoring_status],\n",
        ")\n",
        "\n",
        "# Incident analysis agent\n",
        "incident_agent = Agent(\n",
        "    name=\"Incident Analysis Agent\",\n",
        "    instructions=\"You analyze detected incidents and provide detailed reports about network issues and solutions.\",\n",
        "    handoff_description=\"Specialist for analyzing and reporting video incidents\",\n",
        "    tools=[get_recent_incidents, get_monitoring_status],\n",
        ")\n",
        "\n",
        "# Video display agent\n",
        "display_agent = Agent(\n",
        "    name=\"Video Display Agent\",\n",
        "    instructions=\"You show the live video feed with incident detection overlays.\",\n",
        "    handoff_description=\"Specialist for displaying video feeds with overlays\",\n",
        "    tools=[\n",
        "        show_video_feed_jupyter,\n",
        "        get_monitoring_status,\n",
        "        show_latest_frame,\n",
        "        create_video_summary,\n",
        "    ],\n",
        ")\n",
        "\n",
        "# YouTube search agent\n",
        "youtube_agent = Agent(\n",
        "    name=\"YouTube Stream Agent\",\n",
        "    instructions=\"You search for YouTube livestreams and help set up monitoring.\",\n",
        "    handoff_description=\"Specialist for finding and setting up YouTube livestream monitoring\",\n",
        "    tools=[get_youtube_stream_info, start_video_monitoring],\n",
        ")\n",
        "\n",
        "# Main orchestrator\n",
        "video_orchestrator = Agent(\n",
        "    name=\"Real-time Video Orchestrator\",\n",
        "    instructions=(\n",
        "        \"You coordinate real-time video monitoring and incident detection. \"\n",
        "        \"Use monitoring_agent to start/stop monitoring, incident_agent for analysis, \"\n",
        "        \"display_agent to show video feeds, and youtube_agent for YouTube streams. \"\n",
        "        \"Always provide clear status updates and help users through the complete workflow.\"\n",
        "    ),\n",
        "    tools=[\n",
        "        monitoring_agent.as_tool(\n",
        "            tool_name=\"monitoring\",\n",
        "            tool_description=\"Start, stop, and manage video monitoring\",\n",
        "        ),\n",
        "        incident_agent.as_tool(\n",
        "            tool_name=\"incidents\",\n",
        "            tool_description=\"Analyze and report detected incidents\",\n",
        "        ),\n",
        "        display_agent.as_tool(\n",
        "            tool_name=\"display\",\n",
        "            tool_description=\"Show video feed with incident overlays\",\n",
        "        ),\n",
        "        youtube_agent.as_tool(\n",
        "            tool_name=\"youtube\",\n",
        "            tool_description=\"Search and monitor YouTube livestreams\",\n",
        "        ),\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {},
      "outputs": [],
      "source": [
        "async def quick_start_youtube(youtube_url):\n",
        "    \"\"\"Quick start with YouTube monitoring\"\"\"\n",
        "    print(f\"🚀 Starting YouTube monitoring: {youtube_url}\")\n",
        "\n",
        "    # First test the URL\n",
        "    test_result = await Runner.run(\n",
        "        video_orchestrator, f\"Test this YouTube URL: {youtube_url}\"\n",
        "    )\n",
        "    print(\"Test result:\", test_result.final_output)\n",
        "\n",
        "    # If test passed, try monitoring\n",
        "    if \"test PASSED\" in test_result.final_output:\n",
        "        result = await Runner.run(\n",
        "            video_orchestrator, f\"Start monitoring this YouTube stream: {youtube_url}\"\n",
        "        )\n",
        "        print(\"Response:\", result.final_output)\n",
        "\n",
        "        print(\"\\n🎥 Showing video feed...\")\n",
        "        display_result = await Runner.run(\n",
        "            video_orchestrator, \"Show the video feed with incident overlays\"\n",
        "        )\n",
        "        print(\"Display response:\", display_result.final_output)\n",
        "\n",
        "        return result\n",
        "    else:\n",
        "        print(\"❌ URL test failed, not starting monitoring\")\n",
        "        return test_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {},
      "outputs": [],
      "source": [
        "async def get_youtube_info_example(youtube_url):\n",
        "    \"\"\"Example: Get YouTube stream information\"\"\"\n",
        "    print(f\"🔍 Getting stream info for: {youtube_url}\")\n",
        "\n",
        "    result = await Runner.run(\n",
        "        video_orchestrator, f\"Get stream info for this YouTube URL: {youtube_url}\"\n",
        "    )\n",
        "    print(\"Response:\", result.final_output)\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {},
      "outputs": [],
      "source": [
        "async def video_monitoring_assistant(user_query):\n",
        "    \"\"\"Main video monitoring assistant function matching your pattern\"\"\"\n",
        "    try:\n",
        "        from agents import ItemHelpers, MessageOutputItem, trace\n",
        "    except ImportError:\n",
        "        # Fallback if agents utilities aren't available\n",
        "        result = await Runner.run(video_orchestrator, user_query)\n",
        "        return result.final_output\n",
        "\n",
        "    # First, have the orchestrator determine which tools to use\n",
        "    with trace(\"Video monitoring orchestrator\"):\n",
        "        orchestrator_result = await Runner.run(video_orchestrator, user_query)\n",
        "\n",
        "        # Print intermediate steps for debugging/transparency\n",
        "        print(\"\\n--- Video Monitoring Processing Steps ---\")\n",
        "        for item in orchestrator_result.new_items:\n",
        "            if isinstance(item, MessageOutputItem):\n",
        "                text = ItemHelpers.text_message_output(item)\n",
        "                if text:\n",
        "                    print(f\"  - Processing step: {text}\")\n",
        "\n",
        "        print(\n",
        "            f\"\\n\\n--- Final Video Monitoring Response ---\\n{orchestrator_result.final_output}\"\n",
        "        )\n",
        "        print()\n",
        "\n",
        "    return orchestrator_result.final_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simple test function\n",
        "async def test_video_assistant():\n",
        "    \"\"\"Simple test function to demonstrate usage\"\"\"\n",
        "    print(\"🧪 Testing Video Monitoring Assistant\")\n",
        "\n",
        "    # Test basic status\n",
        "    response1 = await video_monitoring_assistant(\n",
        "        \"What's the current monitoring status?\"\n",
        "    )\n",
        "    print(f\"Status response: {response1}\")\n",
        "\n",
        "    # Test YouTube info\n",
        "    response2 = await video_monitoring_assistant(\n",
        "        \"Get stream info for this YouTube URL: https://www.youtube.com/watch?v=jcEW98mEqmY\"\n",
        "    )\n",
        "    print(f\"YouTube info response: {response2}\")\n",
        "\n",
        "    return response1, response2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "result = await get_youtube_info_example(\"https://www.youtube.com/watch?v=YDfiTGGPYCk\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "responses = await quick_start_youtube(\"https://www.youtube.com/watch?v=YDfiTGGPYCk\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "display_result = await Runner.run(\n",
        "    video_orchestrator, \"Show the video feed with overlays\"\n",
        ")\n",
        "print(display_result.final_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show current frame from NBC livestream\n",
        "frame_result = await Runner.run(video_orchestrator, \"Show the latest frame\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get detailed incident analysis and reports\n",
        "analysis = await Runner.run(video_orchestrator, \"Analyze detected incidents\")\n",
        "print(analysis.final_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get complete monitoring summary\n",
        "summary = await Runner.run(video_orchestrator, \"Create a video summary\")\n",
        "print(summary.final_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "result = await Runner.run(video_orchestrator, \"Show recent incidents\")\n",
        "result.final_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "result = await Runner.run(\n",
        "    video_orchestrator, \"What's the current incident detection status?\"\n",
        ")\n",
        "result.final_output"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "state": {}
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
